{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to supertracker","text":"<p>An easy-to-use library for implementing various multi-object tracking algorithms.</p>"},{"location":"#overview","title":"Overview","text":"<p>Supertracker provides a unified interface for multiple object tracking algorithms, making it easy to implement and switch between different tracking approaches in your computer vision applications.</p>"},{"location":"#available-trackers","title":"Available Trackers","text":""},{"location":"#bytetrack","title":"ByteTrack","text":"<ul> <li>High-performance multi-object tracking</li> <li>Robust occlusion handling</li> <li>Configurable parameters for different scenarios</li> <li>Ideal for real-time applications</li> </ul>"},{"location":"#coming-soon","title":"Coming Soon","text":"<ul> <li>DeepSORT: Deep learning enhanced tracking</li> <li>SORT: Simple online realtime tracking</li> <li>OCSORT: Observation-centric SORT</li> <li>BoT-SORT: Bootstrap your own SORT</li> </ul>"},{"location":"#quick-installation","title":"Quick Installation","text":"<pre><code>pip install supertracker\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from supertracker import ByteTrack\nfrom supertracker import Detections\n\n# Initialize tracker\ntracker = ByteTrack(\n    track_activation_threshold=0.25,\n    lost_track_buffer=30,\n    frame_rate=30\n)\n\n# Process detections\ntracked_objects = tracker.update_with_detections(detections)\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: Basic installation and usage</li> <li>Tutorials: Step-by-step guides for common scenarios</li> <li>API Reference: Detailed documentation of all classes and methods</li> <li>Examples: Real-world implementation examples</li> <li>Configuration: Tracker-specific parameter tuning</li> <li>Contributing: Guidelines for contributing to the project</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>Each tracker implementation is optimized for: - Real-time processing - Memory efficiency - Accuracy in various scenarios - Robust handling of occlusions</p>"},{"location":"#integration-examples","title":"Integration Examples","text":"<p>We provide examples for integration with popular detection frameworks: - YOLO (v5, v6, v7, v8) - Detectron2 - TensorFlow Object Detection API - Custom detection models</p>"},{"location":"#support","title":"Support","text":"<ul> <li>GitHub Issues</li> <li>Documentation</li> <li>Examples Repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"bytetrack/","title":"Bytetrack module","text":""},{"location":"bytetrack/#supertracker.bytetrack.core","title":"<code>core</code>","text":""},{"location":"bytetrack/#supertracker.bytetrack.core.ByteTrack","title":"<code> ByteTrack        </code>","text":"<p>Initialize the ByteTrack object.</p> <p>Parameters:</p> Name Type Description Default <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.</p> <code>0.25</code> <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.</p> <code>30</code> <code>minimum_matching_threshold</code> <code>float</code> <p>Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.</p> <code>0.8</code> <code>frame_rate</code> <code>int</code> <p>The frame rate of the video.</p> <code>30</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.</p> <code>1</code> Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>class ByteTrack:\n    \"\"\"\n    Initialize the ByteTrack object.\n\n    Parameters:\n        track_activation_threshold (float): Detection confidence threshold\n            for track activation. Increasing track_activation_threshold improves accuracy\n            and stability but might miss true detections. Decreasing it increases\n            completeness but risks introducing noise and instability.\n        lost_track_buffer (int): Number of frames to buffer when a track is lost.\n            Increasing lost_track_buffer enhances occlusion handling, significantly\n            reducing the likelihood of track fragmentation or disappearance caused\n            by brief detection gaps.\n        minimum_matching_threshold (float): Threshold for matching tracks with detections.\n            Increasing minimum_matching_threshold improves accuracy but risks fragmentation.\n            Decreasing it improves completeness but risks false positives and drift.\n        frame_rate (int): The frame rate of the video.\n        minimum_consecutive_frames (int): Number of consecutive frames that an object must\n            be tracked before it is considered a 'valid' track.\n            Increasing minimum_consecutive_frames prevents the creation of accidental tracks from\n            false detection or double detection, but risks missing shorter tracks.\n    \"\"\" \n\n    def __init__(\n        self,\n        track_activation_threshold: float = 0.25,\n        lost_track_buffer: int = 30,\n        minimum_matching_threshold: float = 0.8,\n        frame_rate: int = 30,\n        minimum_consecutive_frames: int = 1,\n    ):\n        self.track_activation_threshold = track_activation_threshold\n        self.minimum_matching_threshold = minimum_matching_threshold\n\n        self.frame_id = 0\n        self.det_thresh = self.track_activation_threshold + 0.1\n        self.max_time_lost = int(frame_rate / 30.0 * lost_track_buffer)\n        self.minimum_consecutive_frames = minimum_consecutive_frames\n        self.kalman_filter = KalmanFilter()\n        self.shared_kalman = KalmanFilter()\n\n        self.tracked_tracks: List[STrack] = []\n        self.lost_tracks: List[STrack] = []\n        self.removed_tracks: List[STrack] = []\n\n        # Warning, possible bug: If you also set internal_id to start at 1,\n        # all traces will be connected across objects.\n        self.internal_id_counter = IdCounter()\n        self.external_id_counter = IdCounter(start_id=1)\n\n    def update_with_detections(self, detections: Detections) -&gt; Detections:\n        \"\"\"\n        Updates the tracker with the provided detections and returns the updated\n        detection results.\n\n        Args:\n            detections (Detections): The detections to pass through the tracker.\n\n        \"\"\"\n        tensors = np.hstack(\n            (\n                detections.xyxy,\n                detections.confidence[:, np.newaxis],\n            )\n        )\n        tracks = self.update_with_tensors(tensors=tensors)\n\n        if len(tracks) &gt; 0:\n            detection_bounding_boxes = np.asarray([det[:4] for det in tensors])\n            track_bounding_boxes = np.asarray([track.tlbr for track in tracks])\n\n            ious = box_iou_batch(detection_bounding_boxes, track_bounding_boxes)\n\n            iou_costs = 1 - ious\n\n            matches, _, _ = matching.linear_assignment(iou_costs, 0.5)\n            detections.tracker_id = np.full(len(detections), -1, dtype=int)\n            for i_detection, i_track in matches:\n                detections.tracker_id[i_detection] = int(\n                    tracks[i_track].external_track_id\n                )\n\n            return detections[detections.tracker_id != -1]\n\n        else:\n            detections = Detections.empty()\n            detections.tracker_id = np.array([], dtype=int)\n\n            return detections\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Resets the internal state of the ByteTrack tracker.\n\n        This method clears the tracking data, including tracked, lost,\n        and removed tracks, as well as resetting the frame counter. It's\n        particularly useful when processing multiple videos sequentially,\n        ensuring the tracker starts with a clean state for each new video.\n        \"\"\"\n        self.frame_id = 0\n        self.internal_id_counter.reset()\n        self.external_id_counter.reset()\n        self.tracked_tracks = []\n        self.lost_tracks = []\n        self.removed_tracks = []\n\n    def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n        \"\"\"\n        Updates the tracker with the provided tensors and returns the updated tracks.\n\n        Parameters:\n            tensors: The new tensors to update with.\n\n        Returns:\n            List[STrack]: Updated tracks.\n        \"\"\"\n        self.frame_id += 1\n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n\n        scores = tensors[:, 4]\n        bboxes = tensors[:, :4]\n\n        remain_inds = scores &gt; self.track_activation_threshold\n        inds_low = scores &gt; 0.1\n        inds_high = scores &lt; self.track_activation_threshold\n\n        inds_second = np.logical_and(inds_low, inds_high)\n        dets_second = bboxes[inds_second]\n        dets = bboxes[remain_inds]\n        scores_keep = scores[remain_inds]\n        scores_second = scores[inds_second]\n\n        if len(dets) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections = [\n                STrack(\n                    STrack.tlbr_to_tlwh(tlbr),\n                    score_keep,\n                    self.minimum_consecutive_frames,\n                    self.shared_kalman,\n                    self.internal_id_counter,\n                    self.external_id_counter,\n                )\n                for (tlbr, score_keep) in zip(dets, scores_keep)\n            ]\n        else:\n            detections = []\n\n        \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n        unconfirmed = []\n        tracked_stracks = []  # type: list[STrack]\n\n        for track in self.tracked_tracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                tracked_stracks.append(track)\n\n        \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n        strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n        # Predict the current location with KF\n        STrack.multi_predict(strack_pool, self.shared_kalman)\n        dists = matching.iou_distance(strack_pool, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_track, u_detection = matching.linear_assignment(\n            dists, thresh=self.minimum_matching_threshold\n        )\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id)\n                refind_stracks.append(track)\n\n        \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n        # association the untrack to the low score detections\n        if len(dets_second) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections_second = [\n                STrack(\n                    STrack.tlbr_to_tlwh(tlbr),\n                    score_second,\n                    self.minimum_consecutive_frames,\n                    self.shared_kalman,\n                    self.internal_id_counter,\n                    self.external_id_counter,\n                )\n                for (tlbr, score_second) in zip(dets_second, scores_second)\n            ]\n        else:\n            detections_second = []\n        r_tracked_stracks = [\n            strack_pool[i]\n            for i in u_track\n            if strack_pool[i].state == TrackState.Tracked\n        ]\n        dists = matching.iou_distance(r_tracked_stracks, detections_second)\n        matches, u_track, u_detection_second = matching.linear_assignment(\n            dists, thresh=0.5\n        )\n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections_second[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.state = TrackState.Lost\n                lost_stracks.append(track)\n\n        \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n        detections = [detections[i] for i in u_detection]\n        dists = matching.iou_distance(unconfirmed, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_unconfirmed, u_detection = matching.linear_assignment(\n            dists, thresh=0.7\n        )\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_id)\n            activated_starcks.append(unconfirmed[itracked])\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.state = TrackState.Removed\n            removed_stracks.append(track)\n\n        \"\"\" Step 4: Init new stracks\"\"\"\n        for inew in u_detection:\n            track = detections[inew]\n            if track.score &lt; self.det_thresh:\n                continue\n            track.activate(self.kalman_filter, self.frame_id)\n            activated_starcks.append(track)\n        \"\"\" Step 5: Update state\"\"\"\n        for track in self.lost_tracks:\n            if self.frame_id - track.frame_id &gt; self.max_time_lost:\n                track.state = TrackState.Removed\n                removed_stracks.append(track)\n\n        self.tracked_tracks = [\n            t for t in self.tracked_tracks if t.state == TrackState.Tracked\n        ]\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n        self.lost_tracks.extend(lost_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n        self.removed_tracks = removed_stracks\n        self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n            self.tracked_tracks, self.lost_tracks\n        )\n        output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n        return output_stracks\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.core.ByteTrack.reset","title":"<code>reset(self)</code>","text":"<p>Resets the internal state of the ByteTrack tracker.</p> <p>This method clears the tracking data, including tracked, lost, and removed tracks, as well as resetting the frame counter. It's particularly useful when processing multiple videos sequentially, ensuring the tracker starts with a clean state for each new video.</p> Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Resets the internal state of the ByteTrack tracker.\n\n    This method clears the tracking data, including tracked, lost,\n    and removed tracks, as well as resetting the frame counter. It's\n    particularly useful when processing multiple videos sequentially,\n    ensuring the tracker starts with a clean state for each new video.\n    \"\"\"\n    self.frame_id = 0\n    self.internal_id_counter.reset()\n    self.external_id_counter.reset()\n    self.tracked_tracks = []\n    self.lost_tracks = []\n    self.removed_tracks = []\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.core.ByteTrack.update_with_detections","title":"<code>update_with_detections(self, detections)</code>","text":"<p>Updates the tracker with the provided detections and returns the updated detection results.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to pass through the tracker.</p> required Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>def update_with_detections(self, detections: Detections) -&gt; Detections:\n    \"\"\"\n    Updates the tracker with the provided detections and returns the updated\n    detection results.\n\n    Args:\n        detections (Detections): The detections to pass through the tracker.\n\n    \"\"\"\n    tensors = np.hstack(\n        (\n            detections.xyxy,\n            detections.confidence[:, np.newaxis],\n        )\n    )\n    tracks = self.update_with_tensors(tensors=tensors)\n\n    if len(tracks) &gt; 0:\n        detection_bounding_boxes = np.asarray([det[:4] for det in tensors])\n        track_bounding_boxes = np.asarray([track.tlbr for track in tracks])\n\n        ious = box_iou_batch(detection_bounding_boxes, track_bounding_boxes)\n\n        iou_costs = 1 - ious\n\n        matches, _, _ = matching.linear_assignment(iou_costs, 0.5)\n        detections.tracker_id = np.full(len(detections), -1, dtype=int)\n        for i_detection, i_track in matches:\n            detections.tracker_id[i_detection] = int(\n                tracks[i_track].external_track_id\n            )\n\n        return detections[detections.tracker_id != -1]\n\n    else:\n        detections = Detections.empty()\n        detections.tracker_id = np.array([], dtype=int)\n\n        return detections\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.core.ByteTrack.update_with_tensors","title":"<code>update_with_tensors(self, tensors)</code>","text":"<p>Updates the tracker with the provided tensors and returns the updated tracks.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>ndarray</code> <p>The new tensors to update with.</p> required <p>Returns:</p> Type Description <code>List[STrack]</code> <p>Updated tracks.</p> Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n    \"\"\"\n    Updates the tracker with the provided tensors and returns the updated tracks.\n\n    Parameters:\n        tensors: The new tensors to update with.\n\n    Returns:\n        List[STrack]: Updated tracks.\n    \"\"\"\n    self.frame_id += 1\n    activated_starcks = []\n    refind_stracks = []\n    lost_stracks = []\n    removed_stracks = []\n\n    scores = tensors[:, 4]\n    bboxes = tensors[:, :4]\n\n    remain_inds = scores &gt; self.track_activation_threshold\n    inds_low = scores &gt; 0.1\n    inds_high = scores &lt; self.track_activation_threshold\n\n    inds_second = np.logical_and(inds_low, inds_high)\n    dets_second = bboxes[inds_second]\n    dets = bboxes[remain_inds]\n    scores_keep = scores[remain_inds]\n    scores_second = scores[inds_second]\n\n    if len(dets) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections = [\n            STrack(\n                STrack.tlbr_to_tlwh(tlbr),\n                score_keep,\n                self.minimum_consecutive_frames,\n                self.shared_kalman,\n                self.internal_id_counter,\n                self.external_id_counter,\n            )\n            for (tlbr, score_keep) in zip(dets, scores_keep)\n        ]\n    else:\n        detections = []\n\n    \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n    unconfirmed = []\n    tracked_stracks = []  # type: list[STrack]\n\n    for track in self.tracked_tracks:\n        if not track.is_activated:\n            unconfirmed.append(track)\n        else:\n            tracked_stracks.append(track)\n\n    \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n    strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n    # Predict the current location with KF\n    STrack.multi_predict(strack_pool, self.shared_kalman)\n    dists = matching.iou_distance(strack_pool, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_track, u_detection = matching.linear_assignment(\n        dists, thresh=self.minimum_matching_threshold\n    )\n\n    for itracked, idet in matches:\n        track = strack_pool[itracked]\n        det = detections[idet]\n        if track.state == TrackState.Tracked:\n            track.update(detections[idet], self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id)\n            refind_stracks.append(track)\n\n    \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n    # association the untrack to the low score detections\n    if len(dets_second) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections_second = [\n            STrack(\n                STrack.tlbr_to_tlwh(tlbr),\n                score_second,\n                self.minimum_consecutive_frames,\n                self.shared_kalman,\n                self.internal_id_counter,\n                self.external_id_counter,\n            )\n            for (tlbr, score_second) in zip(dets_second, scores_second)\n        ]\n    else:\n        detections_second = []\n    r_tracked_stracks = [\n        strack_pool[i]\n        for i in u_track\n        if strack_pool[i].state == TrackState.Tracked\n    ]\n    dists = matching.iou_distance(r_tracked_stracks, detections_second)\n    matches, u_track, u_detection_second = matching.linear_assignment(\n        dists, thresh=0.5\n    )\n    for itracked, idet in matches:\n        track = r_tracked_stracks[itracked]\n        det = detections_second[idet]\n        if track.state == TrackState.Tracked:\n            track.update(det, self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id)\n            refind_stracks.append(track)\n\n    for it in u_track:\n        track = r_tracked_stracks[it]\n        if not track.state == TrackState.Lost:\n            track.state = TrackState.Lost\n            lost_stracks.append(track)\n\n    \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n    detections = [detections[i] for i in u_detection]\n    dists = matching.iou_distance(unconfirmed, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_unconfirmed, u_detection = matching.linear_assignment(\n        dists, thresh=0.7\n    )\n    for itracked, idet in matches:\n        unconfirmed[itracked].update(detections[idet], self.frame_id)\n        activated_starcks.append(unconfirmed[itracked])\n    for it in u_unconfirmed:\n        track = unconfirmed[it]\n        track.state = TrackState.Removed\n        removed_stracks.append(track)\n\n    \"\"\" Step 4: Init new stracks\"\"\"\n    for inew in u_detection:\n        track = detections[inew]\n        if track.score &lt; self.det_thresh:\n            continue\n        track.activate(self.kalman_filter, self.frame_id)\n        activated_starcks.append(track)\n    \"\"\" Step 5: Update state\"\"\"\n    for track in self.lost_tracks:\n        if self.frame_id - track.frame_id &gt; self.max_time_lost:\n            track.state = TrackState.Removed\n            removed_stracks.append(track)\n\n    self.tracked_tracks = [\n        t for t in self.tracked_tracks if t.state == TrackState.Tracked\n    ]\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n    self.lost_tracks.extend(lost_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n    self.removed_tracks = removed_stracks\n    self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n        self.tracked_tracks, self.lost_tracks\n    )\n    output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n    return output_stracks\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.core.joint_tracks","title":"<code>joint_tracks(track_list_a, track_list_b)</code>","text":"<p>Joins two lists of tracks, ensuring that the resulting list does not contain tracks with duplicate internal_track_id values.</p> <p>Parameters:</p> Name Type Description Default <code>track_list_a</code> <code>List[supertracker.bytetrack.single_object_track.STrack]</code> <p>First list of tracks (with internal_track_id attribute).</p> required <code>track_list_b</code> <code>List[supertracker.bytetrack.single_object_track.STrack]</code> <p>Second list of tracks (with internal_track_id attribute).</p> required <p>Returns:</p> Type Description <code>List[supertracker.bytetrack.single_object_track.STrack]</code> <p>Combined list of tracks from track_list_a and track_list_b     without duplicate internal_track_id values.</p> Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>def joint_tracks(\n    track_list_a: List[STrack], track_list_b: List[STrack]\n) -&gt; List[STrack]:\n    \"\"\"\n    Joins two lists of tracks, ensuring that the resulting list does not\n    contain tracks with duplicate internal_track_id values.\n\n    Parameters:\n        track_list_a: First list of tracks (with internal_track_id attribute).\n        track_list_b: Second list of tracks (with internal_track_id attribute).\n\n    Returns:\n        Combined list of tracks from track_list_a and track_list_b\n            without duplicate internal_track_id values.\n    \"\"\"\n    seen_track_ids = set()\n    result = []\n\n    for track in track_list_a + track_list_b:\n        if track.internal_track_id not in seen_track_ids:\n            seen_track_ids.add(track.internal_track_id)\n            result.append(track)\n\n    return result\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.core.sub_tracks","title":"<code>sub_tracks(track_list_a, track_list_b)</code>","text":"<p>Returns a list of tracks from track_list_a after removing any tracks that share the same internal_track_id with tracks in track_list_b.</p> <p>Parameters:</p> Name Type Description Default <code>track_list_a</code> <code>List[supertracker.bytetrack.single_object_track.STrack]</code> <p>List of tracks (with internal_track_id attribute).</p> required <code>track_list_b</code> <code>List[supertracker.bytetrack.single_object_track.STrack]</code> <p>List of tracks (with internal_track_id attribute) to be subtracted from track_list_a.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List of remaining tracks from track_list_a after subtraction.</p> Source code in <code>supertracker/bytetrack/core.py</code> <pre><code>def sub_tracks(track_list_a: List[STrack], track_list_b: List[STrack]) -&gt; List[int]:\n    \"\"\"\n    Returns a list of tracks from track_list_a after removing any tracks\n    that share the same internal_track_id with tracks in track_list_b.\n\n    Parameters:\n        track_list_a: List of tracks (with internal_track_id attribute).\n        track_list_b: List of tracks (with internal_track_id attribute) to\n            be subtracted from track_list_a.\n    Returns:\n        List of remaining tracks from track_list_a after subtraction.\n    \"\"\"\n    tracks = {track.internal_track_id: track for track in track_list_a}\n    track_ids_b = {track.internal_track_id for track in track_list_b}\n\n    for track_id in track_ids_b:\n        tracks.pop(track_id, None)\n\n    return list(tracks.values())\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter","title":"<code>kalman_filter</code>","text":""},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter","title":"<code> KalmanFilter        </code>","text":"<p>A simple Kalman filter for tracking bounding boxes in image space.</p> <p>The 8-dimensional state space</p> <pre><code>x, y, a, h, vx, vy, va, vh\n</code></pre> <p>contains the bounding box center position (x, y), aspect ratio a, height h, and their respective velocities.</p> <p>Object motion follows a constant velocity model. The bounding box location (x, y, a, h) is taken as direct observation of the state space (linear observation model).</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>class KalmanFilter:\n    \"\"\"\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, a, h, vx, vy, va, vh\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n    \"\"\"\n\n    def __init__(self):\n        ndim, dt = 4, 1.0\n\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = dt\n        self._update_mat = np.eye(ndim, 2 * ndim)\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Create track from an unassociated measurement.\n\n        Args:\n            measurement (ndarray): Bounding box coordinates (x, y, a, h) with\n                center position (x, y), aspect ratio a, and height h.\n\n        Returns:\n            Tuple[ndarray, ndarray]: Returns the mean vector (8 dimensional) and\n                covariance matrix (8x8 dimensional) of the new track.\n                Unobserved velocities are initialized to 0 mean.\n        \"\"\"\n        mean_pos = measurement\n        mean_vel = np.zeros_like(mean_pos)\n        mean = np.r_[mean_pos, mean_vel]\n\n        std = [\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[3],\n            1e-2,\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            1e-5,\n            10 * self._std_weight_velocity * measurement[3],\n        ]\n        covariance = np.diag(np.square(std))\n        return mean, covariance\n\n    def predict(\n        self, mean: np.ndarray, covariance: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Run Kalman filter prediction step.\n\n        Args:\n            mean (ndarray): The 8 dimensional mean vector of the object\n                state at the previous time step.\n            covariance (ndarray): The 8x8 dimensional covariance matrix of\n                the object state at the previous time step.\n\n        Returns:\n            Tuple[ndarray, ndarray]: Returns the mean vector and\n                covariance matrix of the predicted state.\n                Unobserved velocities are initialized to 0 mean.\n        \"\"\"\n        std_pos = [\n            self._std_weight_position * mean[3],\n            self._std_weight_position * mean[3],\n            1e-2,\n            self._std_weight_position * mean[3],\n        ]\n        std_vel = [\n            self._std_weight_velocity * mean[3],\n            self._std_weight_velocity * mean[3],\n            1e-5,\n            self._std_weight_velocity * mean[3],\n        ]\n        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n\n        mean = np.dot(mean, self._motion_mat.T)\n        covariance = (\n            np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T))\n            + motion_cov\n        )\n\n        return mean, covariance\n\n    def project(\n        self, mean: np.ndarray, covariance: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Project state distribution to measurement space.\n\n        Args:\n            mean (ndarray): The state's mean vector (8 dimensional array).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n\n        Returns:\n            Tuple[ndarray, ndarray]: Returns the projected mean and\n                covariance matrix of the given state estimate.\n        \"\"\"\n        std = [\n            self._std_weight_position * mean[3],\n            self._std_weight_position * mean[3],\n            1e-1,\n            self._std_weight_position * mean[3],\n        ]\n        innovation_cov = np.diag(np.square(std))\n\n        mean = np.dot(self._update_mat, mean)\n        covariance = np.linalg.multi_dot(\n            (self._update_mat, covariance, self._update_mat.T)\n        )\n        return mean, covariance + innovation_cov\n\n    def multi_predict(\n        self, mean: np.ndarray, covariance: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Run Kalman filter prediction step (Vectorized version).\n\n        Args:\n            mean (ndarray): The Nx8 dimensional mean matrix\n                of the object states at the previous time step.\n            covariance (ndarray): The Nx8x8 dimensional covariance matrices\n                of the object states at the previous time step.\n\n        Returns:\n            Tuple[ndarray, ndarray]: Returns the mean vector and\n                covariance matrix of the predicted state.\n                Unobserved velocities are initialized to 0 mean.\n        \"\"\"\n        std_pos = [\n            self._std_weight_position * mean[:, 3],\n            self._std_weight_position * mean[:, 3],\n            1e-2 * np.ones_like(mean[:, 3]),\n            self._std_weight_position * mean[:, 3],\n        ]\n        std_vel = [\n            self._std_weight_velocity * mean[:, 3],\n            self._std_weight_velocity * mean[:, 3],\n            1e-5 * np.ones_like(mean[:, 3]),\n            self._std_weight_velocity * mean[:, 3],\n        ]\n        sqr = np.square(np.r_[std_pos, std_vel]).T\n\n        motion_cov = []\n        for i in range(len(mean)):\n            motion_cov.append(np.diag(sqr[i]))\n        motion_cov = np.asarray(motion_cov)\n\n        mean = np.dot(mean, self._motion_mat.T)\n        left = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\n        covariance = np.dot(left, self._motion_mat.T) + motion_cov\n\n        return mean, covariance\n\n    def update(\n        self, mean: np.ndarray, covariance: np.ndarray, measurement: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Run Kalman filter correction step.\n\n        Args:\n            mean (ndarray): The predicted state's mean vector (8 dimensional).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n            measurement (ndarray): The 4-dimensional measurement vector (x, y, a, h),\n                where (x, y) is the center position, a the aspect ratio,\n                and h the height of the bounding box.\n\n        Returns:\n            Tuple[ndarray, ndarray]: Returns the measurement-corrected\n                state distribution.\n        \"\"\"\n        projected_mean, projected_cov = self.project(mean, covariance)\n\n        chol_factor, lower = scipy.linalg.cho_factor(\n            projected_cov, lower=True, check_finite=False\n        )\n        kalman_gain = scipy.linalg.cho_solve(\n            (chol_factor, lower),\n            np.dot(covariance, self._update_mat.T).T,\n            check_finite=False,\n        ).T\n        innovation = measurement - projected_mean\n\n        new_mean = mean + np.dot(innovation, kalman_gain.T)\n        new_covariance = covariance - np.linalg.multi_dot(\n            (kalman_gain, projected_cov, kalman_gain.T)\n        )\n        return new_mean, new_covariance\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter.initiate","title":"<code>initiate(self, measurement)</code>","text":"<p>Create track from an unassociated measurement.</p> <p>Parameters:</p> Name Type Description Default <code>measurement</code> <code>ndarray</code> <p>Bounding box coordinates (x, y, a, h) with center position (x, y), aspect ratio a, and height h.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Returns the mean vector (8 dimensional) and     covariance matrix (8x8 dimensional) of the new track.     Unobserved velocities are initialized to 0 mean.</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>def initiate(self, measurement: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Create track from an unassociated measurement.\n\n    Args:\n        measurement (ndarray): Bounding box coordinates (x, y, a, h) with\n            center position (x, y), aspect ratio a, and height h.\n\n    Returns:\n        Tuple[ndarray, ndarray]: Returns the mean vector (8 dimensional) and\n            covariance matrix (8x8 dimensional) of the new track.\n            Unobserved velocities are initialized to 0 mean.\n    \"\"\"\n    mean_pos = measurement\n    mean_vel = np.zeros_like(mean_pos)\n    mean = np.r_[mean_pos, mean_vel]\n\n    std = [\n        2 * self._std_weight_position * measurement[3],\n        2 * self._std_weight_position * measurement[3],\n        1e-2,\n        2 * self._std_weight_position * measurement[3],\n        10 * self._std_weight_velocity * measurement[3],\n        10 * self._std_weight_velocity * measurement[3],\n        1e-5,\n        10 * self._std_weight_velocity * measurement[3],\n    ]\n    covariance = np.diag(np.square(std))\n    return mean, covariance\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter.multi_predict","title":"<code>multi_predict(self, mean, covariance)</code>","text":"<p>Run Kalman filter prediction step (Vectorized version).</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The Nx8 dimensional mean matrix of the object states at the previous time step.</p> required <code>covariance</code> <code>ndarray</code> <p>The Nx8x8 dimensional covariance matrices of the object states at the previous time step.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Returns the mean vector and     covariance matrix of the predicted state.     Unobserved velocities are initialized to 0 mean.</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>def multi_predict(\n    self, mean: np.ndarray, covariance: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run Kalman filter prediction step (Vectorized version).\n\n    Args:\n        mean (ndarray): The Nx8 dimensional mean matrix\n            of the object states at the previous time step.\n        covariance (ndarray): The Nx8x8 dimensional covariance matrices\n            of the object states at the previous time step.\n\n    Returns:\n        Tuple[ndarray, ndarray]: Returns the mean vector and\n            covariance matrix of the predicted state.\n            Unobserved velocities are initialized to 0 mean.\n    \"\"\"\n    std_pos = [\n        self._std_weight_position * mean[:, 3],\n        self._std_weight_position * mean[:, 3],\n        1e-2 * np.ones_like(mean[:, 3]),\n        self._std_weight_position * mean[:, 3],\n    ]\n    std_vel = [\n        self._std_weight_velocity * mean[:, 3],\n        self._std_weight_velocity * mean[:, 3],\n        1e-5 * np.ones_like(mean[:, 3]),\n        self._std_weight_velocity * mean[:, 3],\n    ]\n    sqr = np.square(np.r_[std_pos, std_vel]).T\n\n    motion_cov = []\n    for i in range(len(mean)):\n        motion_cov.append(np.diag(sqr[i]))\n    motion_cov = np.asarray(motion_cov)\n\n    mean = np.dot(mean, self._motion_mat.T)\n    left = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\n    covariance = np.dot(left, self._motion_mat.T) + motion_cov\n\n    return mean, covariance\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter.predict","title":"<code>predict(self, mean, covariance)</code>","text":"<p>Run Kalman filter prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The 8 dimensional mean vector of the object state at the previous time step.</p> required <code>covariance</code> <code>ndarray</code> <p>The 8x8 dimensional covariance matrix of the object state at the previous time step.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Returns the mean vector and     covariance matrix of the predicted state.     Unobserved velocities are initialized to 0 mean.</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>def predict(\n    self, mean: np.ndarray, covariance: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run Kalman filter prediction step.\n\n    Args:\n        mean (ndarray): The 8 dimensional mean vector of the object\n            state at the previous time step.\n        covariance (ndarray): The 8x8 dimensional covariance matrix of\n            the object state at the previous time step.\n\n    Returns:\n        Tuple[ndarray, ndarray]: Returns the mean vector and\n            covariance matrix of the predicted state.\n            Unobserved velocities are initialized to 0 mean.\n    \"\"\"\n    std_pos = [\n        self._std_weight_position * mean[3],\n        self._std_weight_position * mean[3],\n        1e-2,\n        self._std_weight_position * mean[3],\n    ]\n    std_vel = [\n        self._std_weight_velocity * mean[3],\n        self._std_weight_velocity * mean[3],\n        1e-5,\n        self._std_weight_velocity * mean[3],\n    ]\n    motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n\n    mean = np.dot(mean, self._motion_mat.T)\n    covariance = (\n        np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T))\n        + motion_cov\n    )\n\n    return mean, covariance\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter.project","title":"<code>project(self, mean, covariance)</code>","text":"<p>Project state distribution to measurement space.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The state's mean vector (8 dimensional array).</p> required <code>covariance</code> <code>ndarray</code> <p>The state's covariance matrix (8x8 dimensional).</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Returns the projected mean and     covariance matrix of the given state estimate.</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>def project(\n    self, mean: np.ndarray, covariance: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Project state distribution to measurement space.\n\n    Args:\n        mean (ndarray): The state's mean vector (8 dimensional array).\n        covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n\n    Returns:\n        Tuple[ndarray, ndarray]: Returns the projected mean and\n            covariance matrix of the given state estimate.\n    \"\"\"\n    std = [\n        self._std_weight_position * mean[3],\n        self._std_weight_position * mean[3],\n        1e-1,\n        self._std_weight_position * mean[3],\n    ]\n    innovation_cov = np.diag(np.square(std))\n\n    mean = np.dot(self._update_mat, mean)\n    covariance = np.linalg.multi_dot(\n        (self._update_mat, covariance, self._update_mat.T)\n    )\n    return mean, covariance + innovation_cov\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.kalman_filter.KalmanFilter.update","title":"<code>update(self, mean, covariance, measurement)</code>","text":"<p>Run Kalman filter correction step.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The predicted state's mean vector (8 dimensional).</p> required <code>covariance</code> <code>ndarray</code> <p>The state's covariance matrix (8x8 dimensional).</p> required <code>measurement</code> <code>ndarray</code> <p>The 4-dimensional measurement vector (x, y, a, h), where (x, y) is the center position, a the aspect ratio, and h the height of the bounding box.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Returns the measurement-corrected     state distribution.</p> Source code in <code>supertracker/bytetrack/kalman_filter.py</code> <pre><code>def update(\n    self, mean: np.ndarray, covariance: np.ndarray, measurement: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run Kalman filter correction step.\n\n    Args:\n        mean (ndarray): The predicted state's mean vector (8 dimensional).\n        covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n        measurement (ndarray): The 4-dimensional measurement vector (x, y, a, h),\n            where (x, y) is the center position, a the aspect ratio,\n            and h the height of the bounding box.\n\n    Returns:\n        Tuple[ndarray, ndarray]: Returns the measurement-corrected\n            state distribution.\n    \"\"\"\n    projected_mean, projected_cov = self.project(mean, covariance)\n\n    chol_factor, lower = scipy.linalg.cho_factor(\n        projected_cov, lower=True, check_finite=False\n    )\n    kalman_gain = scipy.linalg.cho_solve(\n        (chol_factor, lower),\n        np.dot(covariance, self._update_mat.T).T,\n        check_finite=False,\n    ).T\n    innovation = measurement - projected_mean\n\n    new_mean = mean + np.dot(innovation, kalman_gain.T)\n    new_covariance = covariance - np.linalg.multi_dot(\n        (kalman_gain, projected_cov, kalman_gain.T)\n    )\n    return new_mean, new_covariance\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track","title":"<code>single_object_track</code>","text":""},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack","title":"<code> STrack        </code>","text":"Source code in <code>supertracker/bytetrack/single_object_track.py</code> <pre><code>class STrack:\n    def __init__(\n        self,\n        tlwh: npt.NDArray[np.float32],\n        score: npt.NDArray[np.float32],\n        minimum_consecutive_frames: int,\n        shared_kalman: KalmanFilter,\n        internal_id_counter: IdCounter,\n        external_id_counter: IdCounter,\n    ):\n        self.state = TrackState.New\n        self.is_activated = False\n        self.start_frame = 0\n        self.frame_id = 0\n\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.shared_kalman = shared_kalman\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n        self.minimum_consecutive_frames = minimum_consecutive_frames\n\n        self.internal_id_counter = internal_id_counter\n        self.external_id_counter = external_id_counter\n        self.internal_track_id = self.internal_id_counter.NO_ID\n        self.external_track_id = self.external_id_counter.NO_ID\n\n    def predict(self) -&gt; None:\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, self.covariance\n        )\n\n    @staticmethod\n    def multi_predict(stracks: List[STrack], shared_kalman: KalmanFilter) -&gt; None:\n        if len(stracks) &gt; 0:\n            multi_mean = []\n            multi_covariance = []\n            for i, st in enumerate(stracks):\n                multi_mean.append(st.mean.copy())\n                multi_covariance.append(st.covariance)\n                if st.state != TrackState.Tracked:\n                    multi_mean[i][7] = 0\n\n            multi_mean, multi_covariance = shared_kalman.multi_predict(\n                np.asarray(multi_mean), np.asarray(multi_covariance)\n            )\n            for i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\n                stracks[i].mean = mean\n                stracks[i].covariance = cov\n\n    def activate(self, kalman_filter: KalmanFilter, frame_id: int) -&gt; None:\n        \"\"\"Start a new tracklet\"\"\"\n        self.kalman_filter = kalman_filter\n        self.internal_track_id = self.internal_id_counter.new_id()\n        self.mean, self.covariance = self.kalman_filter.initiate(\n            self.tlwh_to_xyah(self._tlwh)\n        )\n\n        self.tracklet_len = 0\n        self.state = TrackState.Tracked\n        if frame_id == 1:\n            self.is_activated = True\n\n        if self.minimum_consecutive_frames == 1:\n            self.external_track_id = self.external_id_counter.new_id()\n\n        self.frame_id = frame_id\n        self.start_frame = frame_id\n\n    def re_activate(self, new_track: STrack, frame_id: int) -&gt; None:\n        self.mean, self.covariance = self.kalman_filter.update(\n            self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n        )\n        self.tracklet_len = 0\n        self.state = TrackState.Tracked\n\n        self.frame_id = frame_id\n        self.score = new_track.score\n\n    def update(self, new_track: STrack, frame_id: int) -&gt; None:\n        \"\"\"\n        Update a matched track\n        :type new_track: STrack\n        :type frame_id: int\n        :type update_feature: bool\n        :return:\n        \"\"\"\n        self.frame_id = frame_id\n        self.tracklet_len += 1\n\n        new_tlwh = new_track.tlwh\n        self.mean, self.covariance = self.kalman_filter.update(\n            self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh)\n        )\n        self.state = TrackState.Tracked\n        if self.tracklet_len == self.minimum_consecutive_frames:\n            self.is_activated = True\n            if self.external_track_id == self.external_id_counter.NO_ID:\n                self.external_track_id = self.external_id_counter.new_id()\n\n        self.score = new_track.score\n\n    @property\n    def tlwh(self) -&gt; npt.NDArray[np.float32]:\n        \"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n        \"\"\"\n        if self.mean is None:\n            return self._tlwh.copy()\n        ret = self.mean[:4].copy()\n        ret[2] *= ret[3]\n        ret[:2] -= ret[2:] / 2\n        return ret\n\n    @property\n    def tlbr(self) -&gt; npt.NDArray[np.float32]:\n        \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n        `(top left, bottom right)`.\n        \"\"\"\n        ret = self.tlwh.copy()\n        ret[2:] += ret[:2]\n        return ret\n\n    @staticmethod\n    def tlwh_to_xyah(tlwh) -&gt; npt.NDArray[np.float32]:\n        \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n        height)`, where the aspect ratio is `width / height`.\n        \"\"\"\n        ret = np.asarray(tlwh).copy()\n        ret[:2] += ret[2:] / 2\n        ret[2] /= ret[3]\n        return ret\n\n    def to_xyah(self) -&gt; npt.NDArray[np.float32]:\n        return self.tlwh_to_xyah(self.tlwh)\n\n    @staticmethod\n    def tlbr_to_tlwh(tlbr) -&gt; npt.NDArray[np.float32]:\n        ret = np.asarray(tlbr).copy()\n        ret[2:] -= ret[:2]\n        return ret\n\n    @staticmethod\n    def tlwh_to_tlbr(tlwh) -&gt; npt.NDArray[np.float32]:\n        ret = np.asarray(tlwh).copy()\n        ret[2:] += ret[:2]\n        return ret\n\n    def __repr__(self) -&gt; str:\n        return \"OT_{}_({}-{})\".format(\n            self.internal_track_id, self.start_frame, self.frame_id\n        )\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack.tlbr","title":"<code>tlbr: npt.NDArray[np.float32]</code>  <code>property</code> <code>readonly</code>","text":"<p>Convert bounding box to format <code>(min x, min y, max x, max y)</code>, i.e., <code>(top left, bottom right)</code>.</p>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack.tlwh","title":"<code>tlwh: npt.NDArray[np.float32]</code>  <code>property</code> <code>readonly</code>","text":"<p>Get current position in bounding box format <code>(top left x, top left y, width, height)</code>.</p>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack.activate","title":"<code>activate(self, kalman_filter, frame_id)</code>","text":"<p>Start a new tracklet</p> Source code in <code>supertracker/bytetrack/single_object_track.py</code> <pre><code>def activate(self, kalman_filter: KalmanFilter, frame_id: int) -&gt; None:\n    \"\"\"Start a new tracklet\"\"\"\n    self.kalman_filter = kalman_filter\n    self.internal_track_id = self.internal_id_counter.new_id()\n    self.mean, self.covariance = self.kalman_filter.initiate(\n        self.tlwh_to_xyah(self._tlwh)\n    )\n\n    self.tracklet_len = 0\n    self.state = TrackState.Tracked\n    if frame_id == 1:\n        self.is_activated = True\n\n    if self.minimum_consecutive_frames == 1:\n        self.external_track_id = self.external_id_counter.new_id()\n\n    self.frame_id = frame_id\n    self.start_frame = frame_id\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack.tlwh_to_xyah","title":"<code>tlwh_to_xyah(tlwh)</code>  <code>staticmethod</code>","text":"<p>Convert bounding box to format <code>(center x, center y, aspect ratio, height)</code>, where the aspect ratio is <code>width / height</code>.</p> Source code in <code>supertracker/bytetrack/single_object_track.py</code> <pre><code>@staticmethod\ndef tlwh_to_xyah(tlwh) -&gt; npt.NDArray[np.float32]:\n    \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n    height)`, where the aspect ratio is `width / height`.\n    \"\"\"\n    ret = np.asarray(tlwh).copy()\n    ret[:2] += ret[2:] / 2\n    ret[2] /= ret[3]\n    return ret\n</code></pre>"},{"location":"bytetrack/#supertracker.bytetrack.single_object_track.STrack.update","title":"<code>update(self, new_track, frame_id)</code>","text":"<p>Update a matched track :type new_track: STrack :type frame_id: int :type update_feature: bool :return:</p> Source code in <code>supertracker/bytetrack/single_object_track.py</code> <pre><code>def update(self, new_track: STrack, frame_id: int) -&gt; None:\n    \"\"\"\n    Update a matched track\n    :type new_track: STrack\n    :type frame_id: int\n    :type update_feature: bool\n    :return:\n    \"\"\"\n    self.frame_id = frame_id\n    self.tracklet_len += 1\n\n    new_tlwh = new_track.tlwh\n    self.mean, self.covariance = self.kalman_filter.update(\n        self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh)\n    )\n    self.state = TrackState.Tracked\n    if self.tracklet_len == self.minimum_consecutive_frames:\n        self.is_activated = True\n        if self.external_track_id == self.external_id_counter.NO_ID:\n            self.external_track_id = self.external_id_counter.new_id()\n\n    self.score = new_track.score\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-2024-02","title":"v0.0.1 - 2024-02","text":"<p>New Features:</p> <ul> <li>Initial release of supertracker library</li> <li>Implemented ByteTrack algorithm</li> <li>Added core Detection class for standardized detection handling</li> <li>Added support for YOLO integration</li> <li>Implemented Kalman filtering for track prediction</li> <li>Added track management system with unique IDs</li> </ul> <p>Improvements:</p> <ul> <li>Optimized tracking performance for real-time applications</li> <li>Implemented efficient track matching algorithms</li> <li>Added comprehensive documentation</li> <li>Created example scripts for common use cases</li> </ul> <p>Technical Details:</p> <ul> <li>Added support for numpy-based detection formats</li> <li>Implemented IOU-based track matching</li> <li>Added track state management (Tracked, Lost, Removed)</li> <li>Included configurable parameters for ByteTrack</li> <li>Added utilities for track manipulation and management</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Hirai-Labs/supertracker/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>supertracker could always use more documentation, whether as part of the official supertracker docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Hirai-Labs/supertracker/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up supertracker for local development.</p> <ol> <li> <p>Fork the supertracker repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/supertracker.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv supertracker\n$ cd supertracker/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 supertracker tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/Hirai-Labs/supertracker/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"detection/","title":"Detection module","text":""},{"location":"detection/#supertracker.detection.core","title":"<code>core</code>","text":""},{"location":"detection/#supertracker.detection.core.Detections","title":"<code> Detections        </code>  <code>dataclass</code>","text":"<p>Detections(xyxy: 'np.ndarray', confidence: 'Optional[np.ndarray]' = None, class_id: 'Optional[np.ndarray]' = None, tracker_id: 'Optional[np.ndarray]' = None)</p> Source code in <code>supertracker/detection/core.py</code> <pre><code>@dataclass\nclass Detections:\n    xyxy: np.ndarray\n    confidence: Optional[np.ndarray] = None\n    class_id: Optional[np.ndarray] = None\n    tracker_id: Optional[np.ndarray] = None\n\n    def __len__(self):\n        return len(self.xyxy)\n\n    def __iter__(self) -&gt; Iterator[Tuple[np.ndarray, Optional[float], Optional[int], Optional[int]]]:\n        for i in range(len(self.xyxy)):\n            yield (\n                self.xyxy[i],\n                self.confidence[i] if self.confidence is not None else None,\n                self.class_id[i] if self.class_id is not None else None,\n                self.tracker_id[i] if self.tracker_id is not None else None\n            )\n\n    def __getitem__(\n        self, index: Union[int, slice, List[int], np.ndarray]\n    ) -&gt; Detections:\n        \"\"\"\n        Get a subset of the Detections object based on the provided index/indices.\n\n        Args:\n            index: Integer, slice, list of integers or numpy array for indexing\n\n        Returns:\n            Detections: A new Detections object containing the subset\n        \"\"\"\n        if isinstance(index, (int, slice, list, np.ndarray)):\n            return Detections(\n                xyxy=self.xyxy[index],\n                confidence=self.confidence[index] if self.confidence is not None else None, \n                class_id=self.class_id[index] if self.class_id is not None else None,\n                tracker_id=self.tracker_id[index] if self.tracker_id is not None else None\n            )\n        else:\n            raise TypeError(f\"Invalid index type: {type(index)}\")\n\n    @classmethod\n    def empty(cls) -&gt; Detections:\n        return cls(\n            xyxy=np.empty((0, 4), dtype=np.float32),\n            confidence=np.array([], dtype=np.float32),\n            class_id=np.array([], dtype=int),\n        )\n</code></pre>"},{"location":"detection/#supertracker.detection.core.Detections.__getitem__","title":"<code>__getitem__(self, index)</code>  <code>special</code>","text":"<p>Get a subset of the Detections object based on the provided index/indices.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, slice, List[int], np.ndarray]</code> <p>Integer, slice, list of integers or numpy array for indexing</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A new Detections object containing the subset</p> Source code in <code>supertracker/detection/core.py</code> <pre><code>def __getitem__(\n    self, index: Union[int, slice, List[int], np.ndarray]\n) -&gt; Detections:\n    \"\"\"\n    Get a subset of the Detections object based on the provided index/indices.\n\n    Args:\n        index: Integer, slice, list of integers or numpy array for indexing\n\n    Returns:\n        Detections: A new Detections object containing the subset\n    \"\"\"\n    if isinstance(index, (int, slice, list, np.ndarray)):\n        return Detections(\n            xyxy=self.xyxy[index],\n            confidence=self.confidence[index] if self.confidence is not None else None, \n            class_id=self.class_id[index] if self.class_id is not None else None,\n            tracker_id=self.tracker_id[index] if self.tracker_id is not None else None\n        )\n    else:\n        raise TypeError(f\"Invalid index type: {type(index)}\")\n</code></pre>"},{"location":"detection/#supertracker.detection.utils","title":"<code>utils</code>","text":""},{"location":"detection/#supertracker.detection.utils.box_iou_batch","title":"<code>box_iou_batch(boxes_true, boxes_detection)</code>","text":"<p>Compute Intersection over Union (IoU) of two sets of bounding boxes -     <code>boxes_true</code> and <code>boxes_detection</code>. Both sets     of boxes are expected to be in <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_true</code> <code>np.ndarray</code> <p>2D <code>np.ndarray</code> representing ground-truth boxes. <code>shape = (N, 4)</code> where <code>N</code> is number of true objects.</p> required <code>boxes_detection</code> <code>np.ndarray</code> <p>2D <code>np.ndarray</code> representing detection boxes. <code>shape = (M, 4)</code> where <code>M</code> is number of detected objects.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Pairwise IoU of boxes from <code>boxes_true</code> and <code>boxes_detection</code>.     <code>shape = (N, M)</code> where <code>N</code> is number of true objects and     <code>M</code> is number of detected objects.</p> Source code in <code>supertracker/detection/utils.py</code> <pre><code>def box_iou_batch(boxes_true: np.ndarray, boxes_detection: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute Intersection over Union (IoU) of two sets of bounding boxes -\n        `boxes_true` and `boxes_detection`. Both sets\n        of boxes are expected to be in `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        boxes_true (np.ndarray): 2D `np.ndarray` representing ground-truth boxes.\n            `shape = (N, 4)` where `N` is number of true objects.\n        boxes_detection (np.ndarray): 2D `np.ndarray` representing detection boxes.\n            `shape = (M, 4)` where `M` is number of detected objects.\n\n    Returns:\n        np.ndarray: Pairwise IoU of boxes from `boxes_true` and `boxes_detection`.\n            `shape = (N, M)` where `N` is number of true objects and\n            `M` is number of detected objects.\n    \"\"\"\n\n    def box_area(box):\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area_true = box_area(boxes_true.T)\n    area_detection = box_area(boxes_detection.T)\n\n    top_left = np.maximum(boxes_true[:, None, :2], boxes_detection[:, :2])\n    bottom_right = np.minimum(boxes_true[:, None, 2:], boxes_detection[:, 2:])\n\n    area_inter = np.prod(np.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n    ious = area_inter / (area_true[:, None] + area_detection - area_inter)\n    ious = np.nan_to_num(ious)\n    return ious\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-supertracker","title":"What is supertracker?","text":"<p>Supertracker is a Python library that provides a unified interface for multiple object tracking algorithms, designed to work seamlessly with various object detection models.</p>"},{"location":"faq/#which-tracking-algorithms-are-supported","title":"Which tracking algorithms are supported?","text":"<p>Currently, ByteTrack is fully implemented. DeepSORT, SORT, OCSORT, and BoT-SORT are planned for future releases.</p>"},{"location":"faq/#what-detection-formats-are-supported","title":"What detection formats are supported?","text":"<p>Supertracker works with any detection format that can provide bounding boxes (xyxy format), confidence scores, and optional class IDs. It has built-in support for YOLO format detections.</p>"},{"location":"faq/#installation","title":"Installation","text":""},{"location":"faq/#why-cant-i-install-supertracker","title":"Why can't I install supertracker?","text":"<p>Make sure you have Python 3.7 or newer installed. Also check that you have numpy and opencv-python in your environment: <pre><code>pip install numpy opencv-python\npip install supertracker\n</code></pre></p>"},{"location":"faq/#does-it-work-with-apple-silicon-m1m2","title":"Does it work with Apple Silicon (M1/M2)?","text":"<p>Yes, supertracker is compatible with Apple Silicon. No special installation steps are required.</p>"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#how-do-i-switch-between-different-trackers","title":"How do I switch between different trackers?","text":"<p>Currently, only ByteTrack is available. When other trackers are implemented, you can switch by importing the desired tracker: <pre><code>from supertracker import ByteTrack  # Currently available\n# from supertracker import DeepSORT  # Coming soon\n# from supertracker import SORT      # Coming soon\n</code></pre></p>"},{"location":"faq/#why-are-my-tracks-getting-lost-quickly","title":"Why are my tracks getting lost quickly?","text":"<p>Check these common issues: 1. Adjust <code>lost_track_buffer</code> for longer track retention 2. Lower <code>track_activation_threshold</code> if detections are weak 3. Ensure consistent frame rate processing</p>"},{"location":"faq/#how-do-i-optimize-for-speed-vs-accuracy","title":"How do I optimize for speed vs accuracy?","text":"<ul> <li>For speed: Lower <code>lost_track_buffer</code>, increase <code>track_activation_threshold</code></li> <li>For accuracy: Increase <code>lost_track_buffer</code>, lower <code>track_activation_threshold</code></li> </ul>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#whats-the-expected-fps","title":"What's the expected FPS?","text":"<p>Performance depends on: - Detection model speed - Image resolution - Hardware capabilities - Number of objects</p> <p>Typical performance on modern hardware: - 30+ FPS at 720p - 20+ FPS at 1080p</p>"},{"location":"faq/#memory-usage","title":"Memory Usage","text":"<p>Typical memory usage: - Base: ~100MB - Per track: negligible (~1KB) - Total: Depends on number of active tracks</p>"},{"location":"faq/#integration","title":"Integration","text":""},{"location":"faq/#can-i-use-it-with-custom-detection-models","title":"Can I use it with custom detection models?","text":"<p>Yes, as long as you can provide detections in the format: <pre><code>detections = Detections(\n    xyxy=boxes,          # numpy array of shape (N, 4)\n    confidence=scores,    # numpy array of shape (N,)\n    class_id=class_ids   # optional, numpy array of shape (N,)\n)\n</code></pre></p>"},{"location":"faq/#does-it-work-with-tensorrt","title":"Does it work with TensorRT?","text":"<p>Yes, supertracker works with any detection model, including TensorRT optimized ones. Just convert the detections to our format.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#common-error-messages","title":"Common Error Messages","text":""},{"location":"faq/#dimension-mismatch-in-detections","title":"\"Dimension mismatch in detections\"","text":"<p>Check that your detection format matches: - xyxy: (N, 4) shape - confidence: (N,) shape - class_id: (N,) shape</p>"},{"location":"faq/#no-tracks-found","title":"\"No tracks found\"","text":"<p>Common causes: 1. Detection confidence too low 2. <code>track_activation_threshold</code> too high 3. No detections being passed to tracker</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-add-a-new-tracker","title":"How can I add a new tracker?","text":"<ol> <li>Fork the repository</li> <li>Implement the tracker following our template</li> <li>Ensure tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"faq/#where-can-i-report-bugs","title":"Where can I report bugs?","text":"<p>Please report bugs on our GitHub Issues page with: 1. Minimal reproduction code 2. Error message 3. Environment details</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install supertracker, run this command in your terminal:</p> <pre><code>pip install supertracker\n</code></pre> <p>This is the preferred method to install supertracker, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install supertracker from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/Hirai-Labs/supertracker\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use supertracker in a project:</p> <pre><code>import supertracker\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>!pip install -q supertracker ultralytics opencv-python\n</pre> !pip install -q supertracker ultralytics opencv-python In\u00a0[2]: Copied! <pre>import cv2\nfrom ultralytics import YOLO\nfrom supertracker import ByteTrack\nfrom supertracker import Detections\n\n# Initialize YOLO and tracker\nmodel = YOLO('yolov8n.pt')\ntracker = ByteTrack(\n    track_activation_threshold=0.25,\n    lost_track_buffer=30,\n    frame_rate=30\n)\n</pre> import cv2 from ultralytics import YOLO from supertracker import ByteTrack from supertracker import Detections  # Initialize YOLO and tracker model = YOLO('yolov8n.pt') tracker = ByteTrack(     track_activation_threshold=0.25,     lost_track_buffer=30,     frame_rate=30 ) <pre>Creating new Ultralytics Settings v0.0.6 file \u2705 \nView Ultralytics Settings with 'yolo settings' or at '/home/runner/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n</pre> <pre>Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n</pre> <pre>\r  0%|          | 0.00/6.25M [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.25M/6.25M [00:00&lt;00:00, 156MB/s]</pre> <pre>\n</pre> In\u00a0[3]: Copied! <pre># Load and process single image\nimage = cv2.imread('example.jpg')\nresults = model(image)[0]\n\n# Convert YOLO results to Detections format\ndetections = Detections(\n    xyxy=results.boxes.xyxy.cpu().numpy(),\n    confidence=results.boxes.conf.cpu().numpy(),\n    class_id=results.boxes.cls.cpu().numpy().astype(int)\n)\n\n# Update tracker\ntracked_objects = tracker.update_with_detections(detections)\n\n# Visualize results\nfor i in range(len(tracked_objects)):\n    box = tracked_objects.xyxy[i].astype(int)\n    track_id = tracked_objects.tracker_id[i]\n    class_id = tracked_objects.class_id[i]\n    conf = tracked_objects.confidence[i]\n    \n    # Draw bounding box\n    cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n    \n    # Draw label\n    label = f\"#{track_id} {model.names[class_id]} {conf:.2f}\"\n    cv2.putText(image, label, (box[0], box[1]-10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\ncv2.waitKey(0)\n</pre> # Load and process single image image = cv2.imread('example.jpg') results = model(image)[0]  # Convert YOLO results to Detections format detections = Detections(     xyxy=results.boxes.xyxy.cpu().numpy(),     confidence=results.boxes.conf.cpu().numpy(),     class_id=results.boxes.cls.cpu().numpy().astype(int) )  # Update tracker tracked_objects = tracker.update_with_detections(detections)  # Visualize results for i in range(len(tracked_objects)):     box = tracked_objects.xyxy[i].astype(int)     track_id = tracked_objects.tracker_id[i]     class_id = tracked_objects.class_id[i]     conf = tracked_objects.confidence[i]          # Draw bounding box     cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)          # Draw label     label = f\"#{track_id} {model.names[class_id]} {conf:.2f}\"     cv2.putText(image, label, (box[0], box[1]-10),                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)  cv2.waitKey(0) <pre>\n</pre> <pre>0: 480x640 1 bicycle, 1 car, 1 truck, 1 dog, 90.2ms\n</pre> <pre>Speed: 2.7ms preprocess, 90.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n</pre> Out[3]: <pre>-1</pre> In\u00a0[4]: Copied! <pre>def process_video(source=0):  # 0 for webcam\n    cap = cv2.VideoCapture(source)\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Run YOLO detection\n        results = model(frame)[0]\n        \n        # Convert to Detections format\n        detections = Detections(\n            xyxy=results.boxes.xyxy.cpu().numpy(),\n            confidence=results.boxes.conf.cpu().numpy(),\n            class_id=results.boxes.cls.cpu().numpy().astype(int)\n        )\n        \n        # Update tracker\n        tracked_objects = tracker.update_with_detections(detections)\n        \n        # Visualize results\n        for i in range(len(tracked_objects)):\n            box = tracked_objects.xyxy[i].astype(int)\n            track_id = tracked_objects.tracker_id[i]\n            class_id = tracked_objects.class_id[i]\n            conf = tracked_objects.confidence[i]\n            \n            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n            label = f\"#{track_id} {model.names[class_id]} {conf:.2f}\"\n            cv2.putText(frame, label, (box[0], box[1]-10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n            \n    cap.release()\n    cv2.destroyAllWindows()\n\n# Run video processing\nprocess_video('vid.mp4')  # or just process_video() for webcam\n</pre> def process_video(source=0):  # 0 for webcam     cap = cv2.VideoCapture(source)          while cap.isOpened():         ret, frame = cap.read()         if not ret:             break          # Run YOLO detection         results = model(frame)[0]                  # Convert to Detections format         detections = Detections(             xyxy=results.boxes.xyxy.cpu().numpy(),             confidence=results.boxes.conf.cpu().numpy(),             class_id=results.boxes.cls.cpu().numpy().astype(int)         )                  # Update tracker         tracked_objects = tracker.update_with_detections(detections)                  # Visualize results         for i in range(len(tracked_objects)):             box = tracked_objects.xyxy[i].astype(int)             track_id = tracked_objects.tracker_id[i]             class_id = tracked_objects.class_id[i]             conf = tracked_objects.confidence[i]                          cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)             label = f\"#{track_id} {model.names[class_id]} {conf:.2f}\"             cv2.putText(frame, label, (box[0], box[1]-10),                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)          if cv2.waitKey(1) &amp; 0xFF == ord('q'):             break                  cap.release()     cv2.destroyAllWindows()  # Run video processing process_video('vid.mp4')  # or just process_video() for webcam <pre>\n</pre> <pre>0: 384x640 37 persons, 2 birds, 75.3ms\n</pre> <pre>Speed: 1.6ms preprocess, 75.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 3 birds, 69.2ms\n</pre> <pre>Speed: 1.8ms preprocess, 69.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 3 birds, 62.5ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 3 birds, 59.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 59.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 2 birds, 60.8ms\n</pre> <pre>Speed: 1.8ms preprocess, 60.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 2 birds, 60.7ms\n</pre> <pre>Speed: 2.5ms preprocess, 60.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 2 birds, 60.6ms\n</pre> <pre>Speed: 2.4ms preprocess, 60.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 2 birds, 65.1ms\n</pre> <pre>Speed: 4.6ms preprocess, 65.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 2 birds, 64.4ms\n</pre> <pre>Speed: 2.0ms preprocess, 64.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 2 birds, 60.8ms\n</pre> <pre>Speed: 6.5ms preprocess, 60.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 60.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 2 birds, 61.6ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 2 birds, 61.7ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 2 birds, 58.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 2 birds, 57.3ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 2 birds, 59.2ms\n</pre> <pre>Speed: 2.8ms preprocess, 59.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 2 birds, 62.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 62.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 2 birds, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 bird, 58.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 traffic light, 1 bird, 1 dog, 62.8ms\n</pre> <pre>Speed: 1.5ms preprocess, 62.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 60.1ms\n</pre> <pre>Speed: 2.4ms preprocess, 60.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 59.2ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 59.2ms\n</pre> <pre>Speed: 3.1ms preprocess, 59.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 56.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 dog, 62.2ms\n</pre> <pre>Speed: 3.5ms preprocess, 62.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 1 bird, 1 dog, 60.1ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 58.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 56.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 62.1ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 59.0ms\n</pre> <pre>Speed: 1.4ms preprocess, 59.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 bird, 62.2ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 bird, 62.1ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 bird, 57.4ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 57.8ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 56.7ms\n</pre> <pre>Speed: 3.9ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 62.3ms\n</pre> <pre>Speed: 3.0ms preprocess, 62.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 bird, 57.5ms\n</pre> <pre>Speed: 1.4ms preprocess, 57.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 56.5ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 56.7ms\n</pre> <pre>Speed: 1.6ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 62.0ms\n</pre> <pre>Speed: 2.7ms preprocess, 62.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 1 bird, 60.3ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 3 birds, 58.0ms\n</pre> <pre>Speed: 1.3ms preprocess, 58.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 2 birds, 59.6ms\n</pre> <pre>Speed: 1.5ms preprocess, 59.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 bird, 61.2ms\n</pre> <pre>Speed: 1.5ms preprocess, 61.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 57.0ms\n</pre> <pre>Speed: 2.6ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 56.9ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 56.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 58.3ms\n</pre> <pre>Speed: 4.4ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 1 dog, 56.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 1 dog, 63.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 63.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 1 bird, 1 dog, 60.0ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 41 persons, 1 bird, 1 dog, 57.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 bird, 2 dogs, 57.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 1 dog, 58.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 58.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 1 dog, 60.7ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 39 persons, 1 bird, 62.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 62.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 bird, 56.8ms\n</pre> <pre>Speed: 2.9ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 58.9ms\n</pre> <pre>Speed: 1.2ms preprocess, 58.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 61.2ms\n</pre> <pre>Speed: 2.8ms preprocess, 61.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 dog, 60.0ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 39 persons, 1 dog, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 62.3ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 dog, 61.1ms\n</pre> <pre>Speed: 1.4ms preprocess, 61.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 dog, 58.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 58.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 39 persons, 1 bird, 57.8ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 bird, 1 dog, 60.4ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 1 bird, 1 dog, 62.5ms\n</pre> <pre>Speed: 1.6ms preprocess, 62.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 3 dogs, 61.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 61.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 39 persons, 1 dog, 60.0ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 59.5ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 57.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 41 persons, 62.8ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 42 persons, 64.8ms\n</pre> <pre>Speed: 1.4ms preprocess, 64.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 1 bird, 1 dog, 58.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 58.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 41 persons, 62.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 62.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 42 persons, 62.1ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 41 persons, 1 bird, 58.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 58.3ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 3 birds, 62.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 62.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 3 birds, 63.0ms\n</pre> <pre>Speed: 1.4ms preprocess, 63.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 41 persons, 60.3ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 60.0ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 61.8ms\n</pre> <pre>Speed: 1.5ms preprocess, 61.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 60.2ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 59.1ms\n</pre> <pre>Speed: 1.4ms preprocess, 59.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 62.1ms\n</pre> <pre>Speed: 3.8ms preprocess, 62.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 58.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 40 persons, 61.4ms\n</pre> <pre>Speed: 2.9ms preprocess, 61.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 62.4ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 61.1ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 58.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 62.3ms\n</pre> <pre>Speed: 2.8ms preprocess, 62.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 61.3ms\n</pre> <pre>Speed: 1.5ms preprocess, 61.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 61.3ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 1 dog, 60.8ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 58.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 58.3ms\n</pre> <pre>Speed: 3.7ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 59.2ms\n</pre> <pre>Speed: 4.1ms preprocess, 59.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 backpack, 60.0ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 dog, 58.9ms\n</pre> <pre>Speed: 1.6ms preprocess, 58.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 57.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 63.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 63.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 62.7ms\n</pre> <pre>Speed: 2.8ms preprocess, 62.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 bird, 61.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 60.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 59.2ms\n</pre> <pre>Speed: 1.5ms preprocess, 59.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 58.7ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 bird, 1 dog, 1 handbag, 1 suitcase, 60.1ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 handbag, 1 suitcase, 59.9ms\n</pre> <pre>Speed: 1.6ms preprocess, 59.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 63.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 63.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 suitcase, 59.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 59.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 suitcase, 58.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 dog, 63.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 63.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 62.5ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 57.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 58.3ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 62.6ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 dog, 62.4ms\n</pre> <pre>Speed: 6.1ms preprocess, 62.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 dog, 56.6ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 63.9ms\n</pre> <pre>Speed: 1.5ms preprocess, 63.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 58.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 37 persons, 1 bird, 58.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 36 persons, 1 bird, 59.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 59.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 38 persons, 1 bird, 60.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 58.5ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 58.3ms\n</pre> <pre>Speed: 1.5ms preprocess, 58.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 bird, 59.9ms\n</pre> <pre>Speed: 1.5ms preprocess, 59.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 60.0ms\n</pre> <pre>Speed: 2.3ms preprocess, 60.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 bird, 58.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 58.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 57.8ms\n</pre> <pre>Speed: 6.1ms preprocess, 57.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 56.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 60.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 60.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 backpack, 59.0ms\n</pre> <pre>Speed: 1.3ms preprocess, 59.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 suitcase, 62.5ms\n</pre> <pre>Speed: 1.5ms preprocess, 62.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 suitcase, 63.6ms\n</pre> <pre>Speed: 2.6ms preprocess, 63.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 suitcase, 58.8ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 57.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 suitcase, 62.3ms\n</pre> <pre>Speed: 5.4ms preprocess, 62.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 63.1ms\n</pre> <pre>Speed: 1.5ms preprocess, 63.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 61.7ms\n</pre> <pre>Speed: 3.3ms preprocess, 61.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 60.5ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 58.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.5ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 horse, 60.7ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 59.0ms\n</pre> <pre>Speed: 1.4ms preprocess, 59.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 62.4ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 58.2ms\n</pre> <pre>Speed: 2.6ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 59.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 59.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 57.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 63.2ms\n</pre> <pre>Speed: 1.2ms preprocess, 63.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 57.3ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 56.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 59.7ms\n</pre> <pre>Speed: 2.0ms preprocess, 59.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 62.6ms\n</pre> <pre>Speed: 3.5ms preprocess, 62.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 62.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 62.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 60.3ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 62.5ms\n</pre> <pre>Speed: 4.5ms preprocess, 62.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 58.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 58.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.2ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.7ms\n</pre> <pre>Speed: 4.2ms preprocess, 58.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 62.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 60.9ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 58.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 58.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 60.1ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 56.9ms\n</pre> <pre>Speed: 2.9ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 60.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 62.2ms\n</pre> <pre>Speed: 1.6ms preprocess, 62.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 62.4ms\n</pre> <pre>Speed: 1.5ms preprocess, 62.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 63.9ms\n</pre> <pre>Speed: 2.2ms preprocess, 63.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 62.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 62.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 56.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 58.8ms\n</pre> <pre>Speed: 2.9ms preprocess, 58.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 61.6ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 57.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 56.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 63.6ms\n</pre> <pre>Speed: 3.4ms preprocess, 63.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 57.9ms\n</pre> <pre>Speed: 2.3ms preprocess, 57.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 56.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 57.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 60.4ms\n</pre> <pre>Speed: 1.4ms preprocess, 60.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 59.8ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 backpack, 57.4ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 59.4ms\n</pre> <pre>Speed: 1.5ms preprocess, 59.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 59.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 59.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 59.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 skis, 57.7ms\n</pre> <pre>Speed: 2.1ms preprocess, 57.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 skis, 58.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 backpack, 60.9ms\n</pre> <pre>Speed: 3.7ms preprocess, 60.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 1 backpack, 60.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 handbag, 57.5ms\n</pre> <pre>Speed: 2.5ms preprocess, 57.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 35 persons, 1 handbag, 63.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 63.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 handbag, 60.5ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 1 dog, 62.7ms\n</pre> <pre>Speed: 1.3ms preprocess, 62.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 56.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 motorcycle, 1 handbag, 57.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 handbag, 58.8ms\n</pre> <pre>Speed: 1.5ms preprocess, 58.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 motorcycle, 1 handbag, 60.4ms\n</pre> <pre>Speed: 1.4ms preprocess, 60.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 horse, 1 handbag, 57.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 2 motorcycles, 1 horse, 1 handbag, 58.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 horse, 1 backpack, 59.9ms\n</pre> <pre>Speed: 1.5ms preprocess, 59.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 60.3ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 motorcycle, 57.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 motorcycle, 1 dog, 57.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 1 backpack, 57.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 motorcycle, 1 backpack, 61.0ms\n</pre> <pre>Speed: 3.4ms preprocess, 61.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 56.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 backpack, 1 handbag, 56.8ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 backpack, 1 handbag, 58.2ms\n</pre> <pre>Speed: 2.3ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 1 backpack, 1 handbag, 60.0ms\n</pre> <pre>Speed: 5.2ms preprocess, 60.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 handbag, 57.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 handbag, 58.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 handbag, 56.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 handbag, 62.7ms\n</pre> <pre>Speed: 2.7ms preprocess, 62.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 handbag, 61.7ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 skis, 59.7ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 skis, 56.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 24 persons, 1 bird, 1 skis, 57.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 bird, 1 horse, 1 handbag, 1 skis, 57.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 bird, 1 handbag, 1 skis, 67.7ms\n</pre> <pre>Speed: 1.5ms preprocess, 67.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 handbag, 1 skis, 58.3ms\n</pre> <pre>Speed: 4.7ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 dog, 1 handbag, 57.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 handbag, 57.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 handbag, 65.9ms\n</pre> <pre>Speed: 1.5ms preprocess, 65.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 2 handbags, 65.1ms\n</pre> <pre>Speed: 4.1ms preprocess, 65.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 horse, 1 backpack, 1 handbag, 66.3ms\n</pre> <pre>Speed: 3.5ms preprocess, 66.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 handbag, 64.5ms\n</pre> <pre>Speed: 3.8ms preprocess, 64.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 handbag, 59.8ms\n</pre> <pre>Speed: 1.3ms preprocess, 59.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 2 dogs, 60.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 60.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 dog, 1 handbag, 60.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 dog, 1 handbag, 63.6ms\n</pre> <pre>Speed: 1.2ms preprocess, 63.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 dog, 1 backpack, 60.4ms\n</pre> <pre>Speed: 5.8ms preprocess, 60.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 backpack, 66.6ms\n</pre> <pre>Speed: 1.6ms preprocess, 66.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 backpack, 69.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 69.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 backpack, 63.7ms\n</pre> <pre>Speed: 2.2ms preprocess, 63.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 1 handbag, 62.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 handbag, 60.6ms\n</pre> <pre>Speed: 2.8ms preprocess, 60.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 bird, 1 handbag, 63.2ms\n</pre> <pre>Speed: 4.1ms preprocess, 63.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 1 motorcycle, 1 handbag, 66.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 66.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 1 motorcycle, 1 bird, 60.1ms\n</pre> <pre>Speed: 1.5ms preprocess, 60.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 1 motorcycle, 1 handbag, 64.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 64.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 24 persons, 2 birds, 1 handbag, 60.3ms\n</pre> <pre>Speed: 3.0ms preprocess, 60.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 2 birds, 1 dog, 59.3ms\n</pre> <pre>Speed: 1.2ms preprocess, 59.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 2 birds, 1 dog, 1 handbag, 65.0ms\n</pre> <pre>Speed: 1.4ms preprocess, 65.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 1 handbag, 64.5ms\n</pre> <pre>Speed: 3.9ms preprocess, 64.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 handbag, 57.7ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 handbag, 62.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 62.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 handbag, 58.4ms\n</pre> <pre>Speed: 2.1ms preprocess, 58.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 1 handbag, 59.9ms\n</pre> <pre>Speed: 2.3ms preprocess, 59.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 56.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 57.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 58.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 backpack, 57.7ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 backpack, 1 handbag, 56.8ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 backpack, 1 handbag, 60.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 60.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 dog, 1 handbag, 64.1ms\n</pre> <pre>Speed: 2.1ms preprocess, 64.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 2 birds, 1 handbag, 57.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 handbag, 58.1ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 58.5ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 57.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 backpack, 56.3ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 1 handbag, 61.6ms\n</pre> <pre>Speed: 1.9ms preprocess, 61.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 1 handbag, 61.2ms\n</pre> <pre>Speed: 3.3ms preprocess, 61.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 60.9ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 backpack, 60.7ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 handbag, 61.2ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 60.0ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 59.9ms\n</pre> <pre>Speed: 1.3ms preprocess, 59.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 1 handbag, 61.5ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 handbag, 58.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 2 handbags, 58.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 3 handbags, 57.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 handbag, 56.7ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 2 handbags, 57.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 handbag, 57.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 dog, 1 handbag, 59.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 59.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 handbag, 57.5ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 56.2ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 58.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 58.0ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 1 dog, 57.2ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 1 dog, 1 backpack, 56.6ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 27 persons, 1 bird, 2 backpacks, 57.4ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.1ms\n</pre> <pre>Speed: 1.6ms preprocess, 60.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 57.9ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 56.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 34 persons, 56.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.2ms\n</pre> <pre>Speed: 5.4ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 backpack, 57.9ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 backpack, 58.0ms\n</pre> <pre>Speed: 2.0ms preprocess, 58.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 1 backpack, 60.8ms\n</pre> <pre>Speed: 2.2ms preprocess, 60.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 bird, 1 dog, 1 backpack, 59.3ms\n</pre> <pre>Speed: 1.3ms preprocess, 59.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 1 bird, 61.3ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 61.8ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 62.1ms\n</pre> <pre>Speed: 4.2ms preprocess, 62.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 61.5ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 bird, 61.8ms\n</pre> <pre>Speed: 3.9ms preprocess, 61.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 60.2ms\n</pre> <pre>Speed: 1.3ms preprocess, 60.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 1 umbrella, 61.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 61.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 bird, 1 dog, 1 umbrella, 61.4ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 2 dogs, 60.5ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 25 persons, 1 dog, 61.8ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 64.5ms\n</pre> <pre>Speed: 1.2ms preprocess, 64.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 61.2ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 61.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 26 persons, 1 dog, 61.6ms\n</pre> <pre>Speed: 1.3ms preprocess, 61.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 dog, 61.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 58.7ms\n</pre> <pre>Speed: 1.9ms preprocess, 58.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 57.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 1 umbrella, 60.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 60.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.5ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 60.7ms\n</pre> <pre>Speed: 1.2ms preprocess, 60.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 61.1ms\n</pre> <pre>Speed: 1.2ms preprocess, 61.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 65.8ms\n</pre> <pre>Speed: 1.3ms preprocess, 65.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 57.9ms\n</pre> <pre>Speed: 2.9ms preprocess, 57.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 57.0ms\n</pre> <pre>Speed: 3.8ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 63.5ms\n</pre> <pre>Speed: 4.4ms preprocess, 63.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 58.3ms\n</pre> <pre>Speed: 2.9ms preprocess, 58.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 57.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 60.0ms\n</pre> <pre>Speed: 2.8ms preprocess, 60.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 30 persons, 1 dog, 57.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 dog, 56.8ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 56.5ms\n</pre> <pre>Speed: 1.9ms preprocess, 56.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 28 persons, 62.3ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 57.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 57.1ms\n</pre> <pre>Speed: 1.9ms preprocess, 57.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 31 persons, 56.6ms\n</pre> <pre>Speed: 2.0ms preprocess, 56.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 55.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 55.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 29 persons, 1 bird, 55.9ms\n</pre> <pre>Speed: 1.5ms preprocess, 55.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 56.0ms\n</pre> <pre>Speed: 1.5ms preprocess, 56.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 33 persons, 56.9ms\n</pre> <pre>Speed: 1.4ms preprocess, 56.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n</pre> <pre>\n</pre> <pre>0: 384x640 32 persons, 1 bird, 62.2ms\n</pre> <pre>Speed: 1.4ms preprocess, 62.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n</pre> In\u00a0[5]: Copied! <pre># Example with custom configuration\ntracker = ByteTrack(\n    track_activation_threshold=0.3,  # Higher threshold for more confident tracks\n    lost_track_buffer=45,           # Longer buffer for better occlusion handling\n    minimum_matching_threshold=0.85, # Stricter matching for better identity preservation\n    frame_rate=30,                  # Match your video frame rate\n    minimum_consecutive_frames=2     # Require more frames for track confirmation\n)\n</pre> # Example with custom configuration tracker = ByteTrack(     track_activation_threshold=0.3,  # Higher threshold for more confident tracks     lost_track_buffer=45,           # Longer buffer for better occlusion handling     minimum_matching_threshold=0.85, # Stricter matching for better identity preservation     frame_rate=30,                  # Match your video frame rate     minimum_consecutive_frames=2     # Require more frames for track confirmation )"},{"location":"examples/intro/#supertracker-introduction","title":"Supertracker Introduction\u00b6","text":"<p>This notebook demonstrates how to use the supertracker library for multi-object tracking.</p>"},{"location":"examples/intro/#installation","title":"Installation\u00b6","text":""},{"location":"examples/intro/#basic-usage-with-yolo","title":"Basic Usage with YOLO\u00b6","text":""},{"location":"examples/intro/#process-single-image","title":"Process Single Image\u00b6","text":""},{"location":"examples/intro/#process-video-stream","title":"Process Video Stream\u00b6","text":""},{"location":"examples/intro/#advanced-configuration","title":"Advanced Configuration\u00b6","text":""}]}